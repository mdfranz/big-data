{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel Computation\n",
    "\n",
    "## Parallel computers\n",
    "- Multiprocessor/multicore: several processors work on data stored in shared memory\n",
    "- Cluster: several processor/memory units work together by exchanging data over a network\n",
    "- Co-processor: a general-purpose processor delegates specific tasks to a special-purpose processor (GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Programming\n",
    "- Decomposition of the complete task into independent subtasks and the data flow between them.\n",
    "- Distribution of the subtasks over the processors minimizing the total execution time.\n",
    "- For clusters: distribution of the data over the nodes minimizing the communication time.\n",
    "- For multiprocessors: optimization of the memory access patterns minimizing waiting times.\n",
    "- Synchronization of the individual processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import sleep\n",
    "def f(x):\n",
    "    sleep(1)\n",
    "    return x*x\n",
    "L = list(range(8))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 884 Âµs, sys: 1.33 ms, total: 2.21 ms\n",
      "Wall time: 8.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum(f(x) for x in L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 ms, sys: 1.55 ms, total: 2.62 ms\n",
      "Wall time: 8.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum(map(f,L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing \n",
    "\n",
    "`multiprocessing` is a package that supports spawning processes.\n",
    "\n",
    "We can use it to display how many concurrent processes you can launch on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Futures\n",
    "\n",
    "The `concurrent.futures` module provides a high-level interface for asynchronously executing callables.\n",
    "\n",
    "The asynchronous execution can be performed with:\n",
    "- **threads**, using ThreadPoolExecutor, \n",
    "- separate **processes**, using ProcessPoolExecutor. \n",
    "Both implement the same interface, which is defined by the abstract Executor class.\n",
    "\n",
    "`concurrent.futures` can't launch **processes** on windows. Windows users must install \n",
    "[loky](https://github.com/tomMoral/loky)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pmap.py\n"
     ]
    }
   ],
   "source": [
    "%%file pmap.py\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from time import sleep, time\n",
    "\n",
    "def f(x):\n",
    "    sleep(1)\n",
    "    return x*x\n",
    "\n",
    "L = list(range(8))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    begin = time()\n",
    "    with ProcessPoolExecutor() as pool:\n",
    "\n",
    "        result = sum(pool.map(f, L))\n",
    "    end = time()\n",
    "    \n",
    "    print(f\"result = {result} and time = {end-begin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = 140 and time = 1.1465930938720703\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} pmap.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `ProcessPoolExecutor` launches one slave process per physical core on the computer. \n",
    "- `pool.map` divides the input list into chunks and puts the tasks (function + chunk) on a queue.\n",
    "- Each slave process takes a task (function + a chunk of data), runs map(function, chunk), and puts the result on a result list.\n",
    "- `pool.map` on the master process waits until all tasks are handled and returns the concatenation of the result lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "CPU times: user 3.7 ms, sys: 3.36 ms, total: 7.06 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "\n",
    "    results = sum(pool.map(f, L))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thread and Process: Differences\n",
    "\n",
    "- A **process** is an instance of a running program. \n",
    "- **Process** may contain one or more **threads**, but a **thread** cannot contain a **process**.\n",
    "- **Process** has a self-contained execution environment. It has its own memory space. \n",
    "- Application running on your computer may be a set of cooperating **processes**.\n",
    "- **Process** don't share its memory, communication between **processes** implies data serialization.\n",
    "\n",
    "- A **thread** is made of and exist within a **process**; every **process** has at least one **thread**. \n",
    "- Multiple **threads** in a **process** share resources, which helps in efficient communication between **threads**.\n",
    "- **Threads** can be concurrent on a multi-core system, with every core executing the separate **threads** simultaneously.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Global Interpreter Lock (GIL)\n",
    "\n",
    "- The Python interpreter is not thread safe.\n",
    "- A few critical internal data structures may only be accessed by one thread at a time. Access to them is protected by the GIL.\n",
    "- Attempts at removing the GIL from Python have failed until now. The main difficulty is maintaining the C API for extension modules.\n",
    "- Multiprocessing avoids the GIL by having separate processes which each have an independent copy of the interpreter data structures.\n",
    "- The price to pay: serialization of tasks, arguments, and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weighted mean and Variance\n",
    "\n",
    "### Exercise 6.1\n",
    "\n",
    "Use `ThreadPoolExecutor` to parallelized functions written in notebook 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [5, 1, 2, 3, 1, 2, 5, 4]\n",
    "P = [0.05, 0.05, 0.15, 0.05, 0.15, 0.2, 0.1, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add, mul\n",
    "from functools import reduce\n",
    "from concurrent.futures import ThreadPoolExecutor as pool\n",
    "\n",
    "def weighted_mean( X, P):\n",
    "    \n",
    "    with pool() as p:\n",
    "        w1 = p.map(mul, X, P)\n",
    "    \n",
    "    return reduce(add,w1)\n",
    "\n",
    "weighted_mean(X,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9600000000000017"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance(X, P):\n",
    "    mu = weighted_mean(X,P)\n",
    "    with pool() as p:\n",
    "        w2 = p.map(lambda x,p:p*x*x, X, P)\n",
    "    return reduce(add,w2) - mu**2\n",
    "\n",
    "variance(X, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(X)\n",
    "p = np.array(P)\n",
    "np.average( x, weights=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9600000000000017"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var =np.sum(p*x**2) - np.average( x, weights=p)**2\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def mapper(filename):\n",
    "    \" split text to list of key/value pairs (word,1)\"\n",
    "    with open(filename) as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    data = data.strip().replace(\".\",\"\").lower().split()\n",
    "        \n",
    "    return sorted([(w,1) for w in data])\n",
    "\n",
    "\n",
    "def partitioner(mapped_values):\n",
    "    \"\"\" get lists from mapper and create a dict with\n",
    "    (word,[1,1,1])\"\"\"\n",
    "    \n",
    "    res = defaultdict(list)\n",
    "    for w, c in mapped_values:\n",
    "        res[w].append(c)\n",
    "        \n",
    "    return res.items()\n",
    "\n",
    "\n",
    "def reducer( item ):\n",
    "    \"\"\" Compute words occurences from dict computed\n",
    "    by partioner\n",
    "    \"\"\"\n",
    "    w, v = item\n",
    "    return (w,len(v))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "## Parallel map\n",
    "\n",
    "\n",
    "- Let's improve the `mapper` function by print out inside the function the current process name. \n",
    "\n",
    "*Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#from loky import ProcessPoolExecutor \n",
    "def process_name(n):\n",
    "    \" prints out the current process name \"\n",
    "    print(mp.current_process().name)\n",
    "\n",
    "with ProcessPoolExecutor() as e:\n",
    "    _ = e.map(process_name, range(mp.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 6.2\n",
    "\n",
    "- Modify the mapper function by adding this print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(filename):\n",
    "    \" split text to list of key/value pairs (word,1)\"\n",
    "    \n",
    "    print(f\"{mp.current_process().name} : {filename}\")\n",
    "    with open(filename) as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    data = data.strip().replace(\".\",\"\").lower().split()\n",
    "        \n",
    "    return sorted([(w,1) for w in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel reduce\n",
    "\n",
    "- For parallel reduce operation, data must be aligned in a container. We already created a `partitioner` function that returns this container.\n",
    "\n",
    "### Exercise 6.3\n",
    "\n",
    "Write a parallel program that uses the three functions above using `ProcessPoolExecutor`. It reads all the \"sample\\*.txt\" files. Map and reduce steps are parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess : sample06.txt\n",
      "MainProcess : sample07.txtMainProcess : sample05.txt\n",
      "\n",
      "MainProcess : sample04.txt\n",
      "MainProcess : sample00.txt\n",
      "MainProcess : sample01.txtMainProcess : sample03.txt\n",
      "MainProcess : sample02.txtMainProcess : sample09.txt\n",
      "\n",
      "MainProcess : sample08.txt\n",
      "MainProcess : sample.txt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('velit', 114),\n",
       " ('non', 109),\n",
       " ('sed', 106),\n",
       " ('modi', 104),\n",
       " ('labore', 103),\n",
       " ('tempora', 103),\n",
       " ('dolore', 100),\n",
       " ('numquam', 99),\n",
       " ('eius', 98),\n",
       " ('etincidunt', 98),\n",
       " ('ipsum', 98),\n",
       " ('quaerat', 98),\n",
       " ('aliquam', 97),\n",
       " ('adipisci', 96),\n",
       " ('voluptatem', 96),\n",
       " ('neque', 95),\n",
       " ('quiquia', 95),\n",
       " ('sit', 95),\n",
       " ('ut', 93),\n",
       " ('magnam', 91),\n",
       " ('quisquam', 91),\n",
       " ('consectetur', 89),\n",
       " ('dolor', 88),\n",
       " ('dolorem', 88),\n",
       " ('amet', 87),\n",
       " ('porro', 79),\n",
       " ('est', 76)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def wordcount(files):\n",
    "    \n",
    "    with ThreadPoolExecutor() as e:\n",
    "    \n",
    "        mapped_values = e.map(mapper, files)\n",
    "        partioned_values = partitioner(chain(*mapped_values))\n",
    "        occurences = e.map(reducer, partioned_values)\n",
    "    \n",
    "    return sorted(occurences,\n",
    "                 key=itemgetter(1),\n",
    "                 reverse=True)\n",
    "    \n",
    "files = glob(\"sample*.txt\")  \n",
    "wordcount(files)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Increase volume of data\n",
    "\n",
    "*Due to the proxy, code above is not runnable on workstations*\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "- [The Latin Library](http://www.thelatinlibrary.com/) contains a huge collection of freely accessible Latin texts. We get links on the Latin Library's homepage ignoring some links that are not associated with a particular author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # web scraping library\n",
    "from urllib.request import *\n",
    "\n",
    "base_url = \"http://www.thelatinlibrary.com/\"\n",
    "home_content = urlopen(base_url)\n",
    "\n",
    "soup = BeautifulSoup(home_content, \"lxml\")\n",
    "author_page_links = soup.find_all(\"a\")\n",
    "author_pages = [ap[\"href\"] for i, ap in enumerate(author_page_links) if i < 49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generate html links\n",
    "\n",
    "- Create a list of all links pointing to Latin texts. The Latin Library uses a special format which makes it easy to find the corresponding links: All of these links contain the name of the text author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ap_content = list()\n",
    "for ap in author_pages:\n",
    "    ap_content.append(urlopen(base_url + ap))\n",
    "\n",
    "book_links = list()\n",
    "for path, content in zip(author_pages, ap_content):\n",
    "    author_name = path.split(\".\")[0]\n",
    "    ap_soup = BeautifulSoup(content, \"lxml\")\n",
    "    book_links += ([link for link in ap_soup.find_all(\"a\", {\"href\": True}) if author_name in link[\"href\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download webpages content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting content 100 of 100\r"
     ]
    }
   ],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "num_pages = 100\n",
    "\n",
    "for i, bl in enumerate(book_links[:num_pages]):\n",
    "    print(\"Getting content \" + str(i + 1) + \" of \" + str(num_pages), end=\"\\r\", flush=True)\n",
    "    try:\n",
    "        content = urlopen(base_url + bl[\"href\"]).read()\n",
    "        with open(f\"book-{i:03d}.dat\",\"wb\") as f:\n",
    "            f.write(content)\n",
    "    except HTTPError as err:\n",
    "        print(\"Unable to retrieve \" + bl[\"href\"] + \".\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extract data files\n",
    "\n",
    "- I already put the content of pages in files named book-*.txt\n",
    "- You can extract data from the archive by running the cell below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```py\n",
    "import os  # library to get directory and file paths\n",
    "import tarfile # this module makes possible to read and write tar archives\n",
    "\n",
    "def extract_data():\n",
    "    datadir = os.path.join('data','latinbooks')\n",
    "    if not os.path.exists(datadir):\n",
    "       print(\"Extracting data...\")\n",
    "       tar_path = os.path.join('data', 'latinbooks.tgz')\n",
    "       with tarfile.open(tar_path, mode='r:gz') as books:\n",
    "          books.extractall('data')\n",
    "            \n",
    "extract_data() # this function call will extract text files in data/latinbooks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = glob('book*.dat')\n",
    "texts = list()\n",
    "for file in files:\n",
    "    with open(file,'rb') as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extract the text from html and split the text at periods to convert it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equisones opilionesque etiam busequae fuit charite nobis fuit misella et quidem casu gravissimo nec vero incomitata manis adivit\n",
      "\n",
      "CPU times: user 1.63 s, sys: 63 ms, total: 1.69 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sentences = list()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(\"Document \" + str(i + 1) + \" of \" + str(len(texts)), end=\"\\r\", flush=True)\n",
    "    textSoup = BeautifulSoup(text, \"lxml\")\n",
    "    paragraphs = textSoup.find_all(\"p\", attrs={\"class\":None})\n",
    "    prepared = (\"\".join([p.text.strip().lower() for p in paragraphs[1:-1]]))\n",
    "    for t in prepared.split(\".\"):\n",
    "        part = \"\".join([c for c in t if c.isalpha() or c.isspace()])\n",
    "        sentences.append(part.strip())\n",
    "\n",
    "# print first and last sentence to check the results\n",
    "print(sentences[0])\n",
    "print(sentences[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 6.4\n",
    "\n",
    "Parallelize this last process using `concurrent.futures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equisones opilionesque etiam busequae fuit charite nobis fuit misella et quidem casu gravissimo nec vero incomitata manis adivit\n",
      "\n",
      "CPU times: user 1.99 s, sys: 581 ms, total: 2.57 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor as pool\n",
    "\n",
    "def sentence_mapper(text):\n",
    "    sentences = list()\n",
    "    textSoup = BeautifulSoup(text, \"lxml\")\n",
    "    paragraphs = textSoup.find_all(\"p\", attrs={\"class\":None})\n",
    "    prepared = (\"\".join([p.text.strip().lower() for p in paragraphs[1:-1]]))\n",
    "    for t in prepared.split(\".\"):\n",
    "        part = \"\".join([c for c in t if c.isalpha() or c.isspace()])\n",
    "        sentences.append(part.strip())\n",
    "    return sentences\n",
    "\n",
    "# parallel map\n",
    "with pool(4) as p:\n",
    "    \n",
    "    mapped_sentences = p.map(sentence_mapper, texts)\n",
    "\n",
    "# reduce\n",
    "sentences = reduce(add, mapped_sentences )\n",
    "\n",
    "# print first and last sentence to check the results\n",
    "print(sentences[0])\n",
    "print(sentences[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [Using Conditional Random Fields and Python for Latin word segmentation](https://medium.com/@felixmohr/using-python-and-conditional-random-fields-for-latin-word-segmentation-416ca7a9e513)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "jupytext": {
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
