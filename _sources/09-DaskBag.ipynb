{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask bag\n",
    "\n",
    "Dask proposes \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "\n",
    "- Dask-bag excels in processing data that can be represented as a sequence of arbitrary inputs (\"messy\" data)\n",
    "- When you encounter a set of data with a format that does not enforce strict structure and datatypes.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "*  [Bag Documenation](http://dask.pydata.org/en/latest/bag.html)\n",
    "*  [Bag API](http://dask.pydata.org/en/latest/bag-api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = list(range(1,9))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "b = db.from_sequence(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b.compute()  # Gather results back to local process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.map(lambda x : x//2).compute() # compute length of each element and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def slow_half( x):\n",
    "    sleep(1)\n",
    "    return x // 2\n",
    "\n",
    "res = b.map(slow_half)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.product(b).compute() # Cartesian product of each pair \n",
    "# of elements in two sequences (or the same sequence in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chain operations to construct more complex computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(b.filter(lambda x: x % 2 > 0)\n",
    "  .product(b)\n",
    "  .filter( lambda v : v[0] % v[1] == 0 and v[0] != v[1])\n",
    "  .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Daily stock example\n",
    "\n",
    "Let's use the bag interface to read the json files containing time series.\n",
    "\n",
    "Each line is a JSON encoded dictionary with the following keys\n",
    "- timestamp: Day.\n",
    "- close: Stock value at the end of the day.\n",
    "- high: Highest value.\n",
    "- low: Lowest value.\n",
    "- open: Opening price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "import os  # library to get directory and file paths\n",
    "import tarfile # this module makes possible to read and write tar archives\n",
    "\n",
    "def extract_data(name, where):\n",
    "    datadir = os.path.join(where,name)\n",
    "    if not os.path.exists(datadir):\n",
    "       print(\"Extracting data...\")\n",
    "       tar_path = os.path.join(where, name+'.tgz')\n",
    "       with tarfile.open(tar_path, mode='r:gz') as data:\n",
    "          data.extractall(where)\n",
    "            \n",
    "extract_data('daily-stock','data') # this function call will extract json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls data/daily-stock/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import json\n",
    "stocks = db.read_text('data/daily-stock/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stocks.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stocks.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "js = stocks.map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "here = os.getcwd() # get the current directory\n",
    "filenames = sorted(glob(os.path.join(here,'data', 'daily-stock', '*.json')))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm data/daily-stock/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "for fn in tqdm(filenames):\n",
    "    with open(fn) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(out_filename, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "filenames = sorted(glob(os.path.join(here,'data', 'daily-stock', '*.h5')))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Serial version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "series = {}\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series[fn] = pd.read_hdf(fn)['close']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in filenames:    # Doubly nested loop over the same collection\n",
    "    for b in filenames:  \n",
    "        if a != b:     # Filter out bad elements\n",
    "            results[a, b] = series[a].corr(series[b])  # Apply function\n",
    "\n",
    "((a, b), corr) = max(results.items(), key=lambda kv: kv[1])  # Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dask.bag methods\n",
    "\n",
    "We can construct most of the above computation with the following dask.bag methods:\n",
    "\n",
    "*  `collection.map(function)`: apply function to each element in collection\n",
    "*  `collection.product(collection)`: Create new collection with every pair of inputs\n",
    "*  `collection.filter(predicate)`: Keep only elements of colleciton that match the predicate function\n",
    "*  `collection.max()`: Compute maximum element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask.bag as db\n",
    "\n",
    "b = db.from_sequence(filenames)\n",
    "series = b.map(lambda fn: pd.read_hdf(fn)['close'])\n",
    "\n",
    "corr = (series.product(series)\n",
    "              .filter(lambda ab: not (ab[0] == ab[1]).all())\n",
    "              .map(lambda ab: ab[0].corr(ab[1])).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = corr.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wordcount with Dask bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lorem\n",
    "\n",
    "lorem.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lorem\n",
    "\n",
    "for i in range(20):\n",
    "    with open(f\"sample{i:02d}.txt\",\"w\") as f:\n",
    "        f.write(lorem.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob('sample*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import glob\n",
    "b = db.read_text(glob.glob('sample*.txt'))\n",
    "\n",
    "wordcount = (b.str.replace(\".\",\" \")  # remove dots\n",
    "             .str.lower()           # lower text\n",
    "             .str.strip()           # remove \\n and trailing spaces\n",
    "             .str.split()           # split into words\n",
    "             .flatten()             # chain all words lists\n",
    "             .frequencies()         # compute occurences\n",
    "             .topk(10, lambda x: x[1])) # sort and return top 10 words\n",
    "\n",
    "\n",
    "wordcount.compute() # Run all tasks and return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Genome example\n",
    "We will use a Dask bag to calculate the frequencies of sequences of five bases, and then sort the sequences into descending order ranked by their frequency.\n",
    "\n",
    "- First we will define some functions to split the bases into sequences of a certain size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exercise 9.1\n",
    "\n",
    "- Implement a function `group_characters(line, n=5)` to group `n` characters together and return a iterator. `line` is a text line in genome.txt file.\n",
    "\n",
    "```py\n",
    ">>> line = \"abcdefghijklmno\"\n",
    ">>> for seq in group_character(line, 5):\n",
    "        print(seq)\n",
    "        \n",
    "\"abcde\"\n",
    "\"efghi\"\n",
    "\"klmno\"\n",
    "```\n",
    "\n",
    "    \n",
    "- Implement `group_and_split(line)`\n",
    "```py\n",
    ">>> group_and_split('abcdefghijklmno')\n",
    "['abcde', 'fghij', 'klmno']\n",
    "```\n",
    "\n",
    "- Use the dask bag to compute  the frequencies of sequences of five bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_path = os.path.join(\"data\")\n",
    "with open(os.path.join(data_path,\"genome.txt\")) as g:\n",
    "    data = g.read()\n",
    "    for i in range(8):\n",
    "        file = os.path.join(data_path,f\"genome{i:02d}.txt\")\n",
    "        with open(file,\"w\") as f:\n",
    "            f.write(data)\n",
    "\n",
    "glob(\"data/genome0*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 9.2\n",
    "\n",
    "The [FASTA](http://www.cbs.dtu.dk/services/NetGene2/fasta.php) file format is used to write several genome sequences.\n",
    "\n",
    "- Create a function that can read a [FASTA file](../data/nucleotide-sample.txt) and compute the frequencies for n = 5 of a given sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 9.3\n",
    "\n",
    "Write a program that uses the function implemented above to read several FASTA files stored in a Dask bag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some remarks about bag\n",
    "\n",
    "*  Higher level dask collections include functions for common patterns\n",
    "*  Move data to collection, construct lazy computation, trigger at the end\n",
    "*  Use Dask.bag (`product + map`) to handle nested for loop\n",
    "\n",
    "Bags have the following known limitations\n",
    "\n",
    "1.  Bag operations tend to be slower than array/dataframe computations in the\n",
    "    same way that Python tends to be slower than NumPy/Pandas\n",
    "2.  ``Bag.groupby`` is slow.  You should try to use ``Bag.foldby`` if possible.\n",
    "    \n",
    "3. Check the [API](http://dask.pydata.org/en/latest/bag-api.html) \n",
    "\n",
    "4. `dask.dataframe` can be faster than `dask.bag`.  But sometimes it is easier to load and clean messy data with a bag. We will see later how to transform a bag into a `dask.dataframe` with the [to_dataframe](http://dask.pydata.org/en/latest/bag-api.html#dask.bag.Bag.to_dataframe) method."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": "0.9",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  },
  "source_map": [
   16,
   30,
   39,
   49,
   57,
   61,
   72,
   77,
   81,
   85,
   90,
   94,
   105,
   118,
   134,
   138,
   148,
   156,
   164,
   173,
   188,
   192,
   208,
   217,
   221,
   241,
   245,
   256,
   273,
   283,
   291,
   295,
   301,
   309,
   313,
   318,
   339,
   346,
   371,
   386,
   394,
   400
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}