{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop File Formats\n",
    "\n",
    "[Format Wars: From VHS and Beta to Avro and Parquet](http://www.svds.com/dataformats/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feather\n",
    "\n",
    "For light data, it is recommanded to use [Feather](https://github.com/wesm/feather). It is a fast, interoperable data frame storage that comes with bindings for python and R.\n",
    "\n",
    "Feather uses also the Apache Arrow columnar memory specification to represent binary data on disk. This makes read and write operations very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "arr = np.random.randn(10000) # 10% nulls\n",
    "arr[::10] = np.nan\n",
    "df = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})\n",
    "feather.write_dataframe(df, 'test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"test.feather\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet file format\n",
    "\n",
    "[Parquet format](https://github.com/apache/parquet-format) is a common binary data store, used particularly in the Hadoop/big-data sphere. It provides several advantages relevant to big-data processing:\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Arrow\n",
    "\n",
    "[Arrow](https://arrow.apache.org/docs/python/) is a columnar in-memory analytics layer designed to accelerate big data. It houses a set of canonical in-memory representations of flat and hierarchical data along with multiple language-bindings for structure manipulation.\n",
    "\n",
    "https://arrow.apache.org/docs/python/parquet.html\n",
    "\n",
    "The Apache Parquet project provides a standardized open-source columnar storage format for use in data analysis systems. It was created originally for use in Apache Hadoop with systems like Apache Drill, Apache Hive, Apache Impala, and Apache Spark adopting it as a shared standard for high performance data IO.\n",
    "\n",
    "Apache Arrow is an ideal in-memory transport layer for data that is being read or written with Parquet files. [PyArrow](https://arrow.apache.org/docs/python/) includes Python bindings to read and write Parquet files with pandas.\n",
    "\n",
    "Example:\n",
    "```py\n",
    "import pyarrow as pa\n",
    "\n",
    "hdfs = pa.hdfs.connect('svmass2.mass.uhb.fr', 54311, 'navaro_p')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'one': [-1, np.nan, 2.5],\n",
    "                   'two': ['foo', 'bar', 'baz'],\n",
    "                   'three': [True, False, True]},\n",
    "                   index=list('abc'))\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pq.read_table('example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.read_table('example.parquet', columns=['one', 'three'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.read_pandas('example.parquet', columns=['two']).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  },
  "source_map": [
   12,
   18,
   26,
   36,
   40,
   54,
   73,
   80,
   88,
   92,
   96,
   100,
   104
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}